\section{Methods} \label{sec:methods}

A pair of experiments was performed to assess the evolvability of the denoising autoencoder and the bottleneck decoder genotype-phenotype maps in relation to the direct genotype-phenotype map.
A high-level overview of these experiments is provide immediately below.
Methodological details can be found in the subsequent subsections.

First, in evolvability signature experiments the novelty and deleteriousness of mutation was assessed under these three genotype-phenotype maps.
Populations of 300 individuals were evolved using a particular map for 50 generations.
Then, the individual with the best fitness score over those 50 generations was isolated.
Mutant offspring of that individual were assessed for phenotypic novelty in relation to their parent and fitness score.
Phenotypic novelty was assessed as the euclidean distance between a parent phenotype $\vec{p}$ and a child phenotype $\vec{x}$.
Fitness was calculated according to the same criteria used during the 50 generations of evolution (described below).
Forty replicate evolutionary runs were performed for each genotype-phenotype map.
For each replicate run, 10,000 mutant offspring of the sampled champion were assessed.
The novelty and fitness scores of these mutant offspring (pooled over all 40 replicate runs) was used to generate the evolvability signatures presented in Section \ref{sec:results} \cite{tarapore2015evolvability}.

Second, experiments were performed to assess the ability of the three genotype-phenotype maps to facilitate traversal of the evolutionary search space in response to a selective pressure.
Specifically, a selective pressure for short (i.e. zero) table height was added.
Table height is calculated as the mean leg length of a table.
For each map, three replicate populations of 300 individuals were evolved for 5,000 generations.
These populations were initialized to have table height of approximately 1,000.
Response to selective pressure for short table height was assessed by tracking mean table height of the populations generation-by-generation.

\subsection{Bottleneck Autoencoder ANN Architecture and Training}

The bottleneck autoencoder was composed of two architectural components: an encoder and a decoder.
The encoder consisted of a 100-to-1 fully-connected linear layer with bias.
The decoder consisted of a 1-to-100 fully-connected linear layer with bias.
Thus, the bottleneck consisted of 1 float value.

The was network was trained for 200 epochs by stochastic gradient descent with parameter settings:
\begin{itemize}
  \item learning rate: $1^{-3}$
  \item momentum: $0.7$
  \item batch size: $16$
\end{itemize}
Loss was defined as mean square error of the difference between the presented phenotype and output (the reconstructed phenotype).

Training data for the bottleneck encoder was generated from 250 populations of 300 individuals evolved with direct genotype-phenotype map.
Initial leg lengths of each separate population were taken from a Gaussian random walk seeded uniformly between $0$ and $1000$ with $\mu = 0, \sigma = 1.0$ where each 100 consecutive values was taken as the initial leg lengths of a particular individual.
This step was performed to ensure that the 250 populations were well-spread throughout the phenotype space.
The 250 populations were evolved separately for 50 generations using the operators and settings described for the direct encoding for the evolvability signature experiments.
The 7500 phenotypes --- vectors of 100 float values --- present in the populations after 50 generations of evolution were taken as training data.
Leg lengths in the phenotypes were normalized to the range $(0,1)$ for the training process.

\subsection{Denoising Autoencoder ANN Architecture and Training}

The denoising autoencoder consisted of a 100-to-100 fully-connected linear layer without bias.

The was network was trained for 2500 epochs by stochastic gradient descent with parameter settings:
\begin{itemize}
  \item learning rate: $1^{-4}$
  \item momentum: $0.9$
  \item batch size: $2048$
\end{itemize}
Model parameters were initialized uniformly between $0.005$ and $0.015$.
During training, parameters were clamped in the range $(0,1)$.
During the training process, Gaussian noise with $\mu = 0, \sigma = 0.025$ was introduced to the input presented to the autoencoder.
Loss was defined as mean square error of the difference between the original phenotype without Gaussian noise and output (the reconstructed phenotype).

The same training data generated for the bottleneck autoencoder was used to train the denoising encoder.

\subsection{Genotype-Phenotype Maps}

Three genotype-phenotype maps were used: the direct genotype-phenotype map, the bottleneck decoder encoding, and the denoising encoding.
In these experiments, the phenotype is 100 floats (representing 100 individual leg lengths).
Thus, for all three maps, the phenotype space is $\mathcal{R}^{100}$.

The direct genotype-phenotype map is the identity function so under the direct map, the genotype space is $\mathcal{R}^{100}$.

Under the bottleneck map, just the decoder component of the full bottleneck autoencoder is used as the genotype-phenotype map.
The bottleneck autoencoder used in these experiments narrows down to a single node layer, so under this map the genotype space is $\mathcal{R}$.

Under the denoising encoding, the full denoising autoencoder is used.
The shape of input to the denoising autoencoder is identical to the shape of the phenotype (a hundred-float vector), so under this map the genotype space is $\mathcal{R}^{100}$.

\subsection{Operators}

For all experiments, tournament selection with $k = 5$ was used.
Two-point crossover was also used in all experiments.
(Note, though, that when the bottleneck genotype-phenotype is was employed the genotype is a member of $\mathcal{R}$ so crossover has no effect.)
All offspring engaged in crossover with one other individual with a probability $0.5$.
Mutation was performed by site-wise Gaussian perturbation of the genome.
For evolvability signature experiments, mutation of genome elements was $\mu=0, \sigma=0.1$ with a per-individual probability of $0.2$ and a subsequent per-site probability of $0.01$.
For response to selection experiments, mutation of genome elements was $\mu=0, \sigma=0.1$ with a per-individual probability of $0.2$ and a per-site probability of $0.2$.
(For all experiments with the bottleneck map, a per-site probability of $1$ $\sigma=0.1$ was employed.)

These particular operators were used due to their easy accessibility in DEAP.
As these experiments were demonstrative instead of applied, the primary criteria
for operators and parameters needed to meet was simply not being obscure enough to distract from the new genotype-phenotype mapping techniques being demonstrated.
It would be beneficial to repeat these experiments with other operator and parameter choices popular in the literature and in applied use to investigate the generalizability of those new techniques.


\subsection{Evaluation Functions}

For evolvability signature experiments, ``level-ness'' was the sole criterion determining fitness score.
For a phenotype $\vec{x}$, the fitness score was computed as
\begin{align*}
-\sigma(\vec{x})
\end{align*}
where $\sigma$ represents calculation of standard deviation.
Note that in all experiments performed, selection was performed to maximize (not minimize) fitness score.
Under this criterion, each level table of a particular height is a local fitness peak because any single-site mutation increases the leg height variance.
This fitness criteria was chosen for evolvability signature experiments because of its ruggedness.

For response to selection experiments, the ``level-ness'' and absolute height of a table were both factored into the fitness score.
For a phenotype $\vec{x}$, the fitness score was computed as
\begin{align*}
-\sigma(\vec{x}) - |\mu(\vec{x})/10|
\end{align*}
where $\sigma$ represents calculation of standard deviation and $\mu$ represents calculation of mean.
Under this criterion, a selection pressure for short tables is applied.
The global fitness score peak is the table with all legs length 0.
However, the phenotypic fitness landscape remains rugged with local peaks occurring as before at level tables.
Thus, the ability of genotype-phenotype map to facilitate evolution will be reflected by the ability of a population to escape local fitness peaks and progress towards the global fitness peak.

\subsection{Implementation}

The evolutionary computing components of this project were implemented using the Distributed Evolutionary Algorithms for Python package, which allows for rapid prototyping and extreme flexibility \cite{fortin2012deap}.
The artificial neural network autoencoder components of this project were implemented using PyTorch, a Python-based deep learning framework \cite{paszke2017pytorch}.

Experiments were run on a PC (no GPU).
In total, the experiments reported used approximately 12 hours of compute time.

Code components for this project are available at \url{https://github.com/mmore500/cse-848-project}.
